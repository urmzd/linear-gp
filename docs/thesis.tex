\documentclass[12pt, final]{dalcsthesis}
\usepackage{graphicx}
\usepackage{subcaption}
\usepackage{amsmath}
\usepackage{algorithm}
\usepackage[noend]{algpseudocode}
\usepackage{hyperref}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{mathptmx}
% \usepackage[scaled=.90]{helvet}
\usepackage{courier}
\usepackage{array}
\usepackage[cache=false]{minted}
\usepackage{csvsimple-l3}
\usepackage{longtable}
\usepackage{tablefootnote}

\graphicspath{{../assets/experiments/aggregate_results/figures}{../assets/experiments/baseline/figures}}

\begin{document}
\bcshon
\title{Reinforced Linear Genetic Programming}
\author{Urmzd Mukhammadnaim}
\defenceday{12}
\defencemonth{April}
\defenceyear{2023}
\convocation{May}{2023}

\supervisor{M. Heywood}
\reader{N. Zincir-Heywood}

\renewcommand{\listoflistings}{%
  \cleardoublepage
  \addcontentsline{toc}{chapter}{\listoflistingscaption}%
  \listof{listing}{\listoflistingscaption}%
}

\frontmatter
\addcontentsline{toc}{chapter}{List of Algorithms}
\listofalgorithms
\listoflistings

\begin{abstract}
	Linear Genetic Programming (LGP) is a powerful technique that allows for a variety of problems to be solved using a linear representation of programs. However, there still exists some limitations to the technique, such as the need for humans to explicitly map registers to actions. This thesis proposes a novel approach that uses Q-Learning on top of LGP, Reinforced Linear Genetic Programming (RLGP) to learn the optimal register-action assignments. In doing so, we introduce a new framework `linear-gp' written in memory-safe Rust that allows for extensive experimentation for future works.
\end{abstract}

\begin{acknowledgements}
	This work is lovingly dedicated to all the wonderful people in my life who have supported me in my educational endeavors. My deepest gratitude goes to my brother Frebarz, who consistently encouraged me to reach new heights. To my mother, Khalima, as well as my sisters Rita and Rukhafzo, who have always been my motivation and continue to fuel my ambitions. And to my late father Muhammad Naim Nazar Muhammad (09/25/1958 - 06/29/2006), whose cherished memory and unwavering dedication to his passions have inspired my own journey.

	I also want to express my heartfelt appreciation to my dear friends, who have played an integral role in this adventure. Your friendship, understanding, and steadfast support have been invaluable as I navigated the challenges and victories of my academic pursuits. Each of you has been a constant source of encouragement, inspiration, and motivation. So, a heartfelt thank you to Kerel, Colin, Mohammed, Efaj, Assib, Jason, Chris, and everyone else who has been an active and cherished part of my life.

	Furthermore, I would like to extend a warm dedication to my young niece Eva. It brings me joy to see your curiosity and enthusiasm grow as you learn about the world around you. My hope is that my own journey inspires you to embrace your dreams and keep exploring new possibilities. Eva, remember that with determination, a curious mind, and the love and support of your family, you can achieve great things. Keep learning, and know that I will always be rooting for you.

	Finally, I am immensely grateful to Dr.\ Heywood for his patience and unwavering guidance throughout the entire process of this thesis, from its inception to completion. His mentorship has allowed me to immerse myself in the world of academia and further ignite my passion for learning and discovery. To each and every one of you who have played a significant role in my academic journey, I offer my sincere thanks and appreciation. Your love and support mean the world to me.
\end{acknowledgements}

\mainmatter

\chapter{Introduction}
The rapid growth of technology and the increasing complexity of real-world problems demand efficient and effective optimization techniques. Evolutionary algorithms (EAs) have demonstrated significant potential in addressing these challenges by emulating the process of natural selection to search for optimal solutions \cite{poli08}. Linear Genetic Programming (LGP), a branch of Genetic Programming (GP), is one such EA that has gained attention in the field of computer science for its unique approach to tackling complex problems \cite{song03}. By representing programs as a sequence of linear instructions, similar to what is found when programming imperatively, LGP allows for solutions that are not only potentially more interpretable but also more efficient to execute and manipulate.
The resulting algorithm, Reinforced Linear Genetic Programming (RLGP), is then able to learn the optimal register-action assignments, potentially leading to more efficient and effective solutions. RLGP inherits the benefits of LGP, such as its linear representation and ease of manipulation while augmenting it with the adaptive capabilities of Q-Learning. This integration allows RLGP to explore and exploit the solution space more effectively, resulting in improved performance when addressing complex optimization problems.
We evaluate the performance of RLGP on a variety of benchmark problems, including the cartpole-v1 and mountain-car-v0 environments from the OpenAI Gym library \cite{1606.01540}. These environments provide challenging tasks that require sophisticated decision-making and control strategies, making them suitable for assessing the effectiveness of RLGP. We compare the baseline LGP framework with the augmented RLGP framework in terms of solution quality, convergence speed, and adaptability to dynamic problem domains.
By combining the strengths of LGP and Q-Learning, RLGP might represent a significant advancement in the field of evolutionary computation. This research not only contributes to the understanding of hybrid evolutionary algorithms but also paves the way for future work on incorporating reinforcement learning techniques into other EAs.

\chapter{Background}
\section{Linear Genetic Programming}
Linear Genetic Programming (LGP) is an advanced form of Genetic Programming (GP), a powerful machine learning technique introduced by Brameier and Banzhaf in their seminal work \cite{brameier2001comparison}. Unlike the traditional tree-based GP, LGP represents programs as a linear sequence of instructions, similar to assembly language or machine code \cite{koza93}. This representation offers several advantages, such as improved evolvability, efficient execution, and simplicity of crossover and mutation operations.
In LGP, programs are composed of registers and instructions, where each instruction manipulates the contents of registers using arithmetic, logical, or conditional operations. The evolutionary process is similar to that of canonical GP, but involves variation operators that act directly upon a linear set of instructions. The variations are as follows, reproduction (cloning), recombination (breeding) and mutation.
In their paper, Brameier and Banzhaf compared the performance of LGP to that of traditional GP and neural networks. They discovered that LGP had classification and generalization capabilities that were comparable to those of neural networks \cite{brameier2001comparison}. This finding was significant, as it demonstrated that LGP could serve as an alternative to neural networks for solving complex machine learning problems. Moreover, LGP's linear representation allows for more interpretable solutions, which is an important consideration in many applications where understanding the underlying model is crucial.
The paper further explored the benefits of LGP's linear representation, such as improved evolvability and more efficient execution. These advantages make LGP an attractive choice for solving complex problems, as it can produce high-quality solutions more quickly than traditional GP methods. Additionally, the simplicity of crossover and mutation operations in LGP ensures that the evolutionary process remains efficient and effective, allowing for the exploration of a diverse range of solutions.
Brameier and Banzhaf's work on LGP laid the foundation for further research into the capabilities and applications of this powerful machine learning technique. The findings of their paper highlight the potential of LGP as a robust and versatile machine learning approach, particularly in scenarios where both high performance and interpretability are required.
\section{Reinforced Genetic Programming}
In the RGP paper, Downing applied the Reinforced Genetic Programming approach to several benchmark problems, including function optimization and control tasks \cite{downing1995reinforced}. The results showed that RGP significantly outperformed traditional Genetic Programming and other baseline algorithms in terms of convergence speed, solution quality, and robustness. This demonstrated the effectiveness of incorporating Q-Learning into the genetic programming framework to guide the exploration and exploitation of the search space.
One of the key findings of the paper was that the combination of Genetic Programming and Q-Learning allowed RGP to adapt more efficiently to changing environments and problem landscapes. By utilizing the reinforcement signals from the environment, RGP could dynamically adjust its search strategy, making it more responsive to the changes in the problem domain. This ability to adapt and learn from the environment is particularly relevant to real-world problems, where the solution space may be dynamic, noisy, or uncertain.
The paper also introduced several novel techniques for integrating Q-Learning into the Genetic Programming framework, such as the use of Q-values to bias the selection of genetic operations and the incorporation of reinforcement signals into the fitness function. These innovations allowed RGP to leverage the strengths of both Genetic Programming and Q-Learning, resulting in a more powerful and flexible optimization algorithm.
In the context of Reinforced Linear Genetic Programming (RLGP), the findings of Downing's paper suggest that integrating Q-Learning into the LGP framework could yield similar benefits. By combining the global search capabilities of LGP with the local search and adaptation of Q-Learning, the resulting algorithm, which could be referred to as RLGP, may be able to tackle complex and dynamic problem domains more effectively.
It is important to note that RGP represents programs as decision trees, while LGP uses linear sequences of instructions for program representation. This fundamental difference necessitates adopting a distinct approach when incorporating Q-learning into LGP.
\chapter{Methodology}

\section{Framework Overview}
We utilize the Rust programming language to develop a flexible and extensible framework for our research, available at \url{https://github.com/urmzd/linear-gp}. The framework is specifically designed to address a wide range of problems using LGP and RLGP while offering extensive configurability for experimentation purposes. The framework is composed of multiple engines, each implementing core functionalities essential for the evolutionary process. The primary engines we focus on are described below:

\begin{itemize}
	\item \textbf{Core}: This engine establishes the fundamental processes the framework employs to evolve a population of individuals. It initializes the population, iteratively evaluates the fitness of individuals, performs selection, and applies genetic operations such as mutation and crossover to generate offspring.

	\item \textbf{Breed}: This engine is responsible for defining the genetic operators that facilitate the exchange of genetic material between individuals. In particular, it outlines how individuals undergo crossover, generating offspring with a combination of their parents' genetic code.

	\item \textbf{Mutate}: This engine focuses on the stochastic modification of single individuals within the population. It dictates how genetic alterations are applied to individuals, potentially leading to the discovery of novel and improved solutions.

	\item \textbf{Fitness}: The Fitness engine is tasked with evaluating the performance of each individual in the population. It measures how well an individual can solve the target problem, providing a basis for selection and guiding the evolutionary search process.

	\item \textbf{Generate}: This engine is concerned with the stochastic creation of individuals and environments. It defines the methods for generating new individuals with random genetic material, as well as the procedures for initializing problem-specific environments that influence the evaluation of individuals.
\end{itemize}

\subsection{Framework Configuration \& Hyperparameters}

The available framework configuration properties are listed in \ref{tab:hyperparameters}. Note that note all configurations are always used.
For instance, programs that do not use Q-Learning during their fitness evaluation, $H_{alpha}$, $H_{gamma}$, $H_{epsilon}$, $H_{alpha\_decay}$ or $H_{epsilon\_decay}$ is not required.
\begin{itemize}
	\item $H$ denotes the set of hyperparameters.
	\item $H_{p}$ denotes the set of hyperparameters that are related to a specific property $p$.
\end{itemize}
For example, when referencing gap, we use the notation $H_{gap}$.

\begin{table}[!htb]
	\centering
	\begin{tabular}{|l|l|l|}
		\hline
		\textbf{Property}  & \textbf{Type} & \textbf{Value Range} \\
		\hline
		default\_fitness   & float         & -                    \\
		\hline
		population\_size   & int           & [0,)                 \\
		\hline
		gap                & float         & (0, 1.)              \\
		\hline
		mutation\_percent  & float         & [0, 1.0]             \\
		\hline
		crossover\_percent & float         & [0, 1.0]             \\
		\hline
		n\_generations     & int           & [0,)                 \\
		\hline
		n\_trials          & int           & [1,)                 \\
		\hline
		seed               & int or None   & [0,)                 \\
		\hline
		max\_instructions  & int           & [1,)                 \\
		\hline
		n\_extras          & int           & [1,)                 \\
		\hline
		external\_factor   & float         & [0,)                 \\
		\hline
		n\_actions         & int           & -                    \\
		\hline
		n\_inputs          & int           & -                    \\
		\hline
		alpha              & float         & [0, 1.0]             \\
		\hline
		gamma              & float         & [0, 1.0]             \\
		\hline
		epsilon            & float         & [0, 1]               \\
		\hline
		alpha\_decay       & float         & [0, 1]               \\
		\hline
		epsilon\_decay     & float         & [0, 1]               \\
		\hline
	\end{tabular}
	\caption{Hyperparameters}
	\label{tab:hyperparameters}
\end{table}
The genetic algorithm involves several hyperparameters that influence its behavior. \texttt{default\_fitness} is the default fitness value assigned to individuals in the population when there is an issue with their fitness evaluation. The number of individuals in each generation is determined by \texttt{population\_size}. The \texttt{gap} parameter represents the percentage of the population that is replaced with offspring in each generation. The \texttt{mutation\_percent} and \texttt{crossover\_percent} values control the proportions of individuals generated through mutation and crossover processes, respectively.

The algorithm runs for a set number of generations, specified by \texttt{n\_generations}. During fitness evaluation, a program must interact with an environment a certain number of times, determined by \texttt{n\_trials}, and the results are aggregated to produce a final value. The pseudorandom number generator (PRNG) is initialized with a seed, \texttt{seed}, or noise from the system if set to None. The genetic program has a maximum number of instructions, \texttt{max\_instructions}, and a certain number of working registers, specified by \texttt{n\_extras}.

The \texttt{external\_factor} parameter adjusts the amplification or reduction of inputs from external sources. The program has \texttt{n\_actions} possible choices or action registers, while the number of feature values for an input from an external source is determined by \texttt{n\_inputs}.

The Q-learning algorithm also utilizes various hyperparameters. The initial learning rate, \texttt{alpha}, sets the pace for updating action value estimates. The \texttt{gamma} parameter acts as the discount factor for future rewards, affecting the balance between immediate and long-term rewards. The probability of selecting a random action is controlled by \texttt{epsilon}, allowing the algorithm to explore the state space. Over time, the learning rate and exploration rate decay at rates specified by \texttt{alpha\_decay} and \texttt{epsilon\_decay}, respectively, allowing a shift from exploration to exploitation.

\subsection{Program Representation}
A program can be thought of as a container holding instructions and a set of registers. A single instruction consists of the source register index, the target register index, the operation to be performed and a mode flag. The mode flag is used to determine where the source register is located. If set, the mode flag indicates that the target register is located outside the program. In other words, the value held in the target register is given by an external source (such as the features of some dataset or the environment state). We can represent the program as a sequence of instructions in the following format.
\begin{equation}
	R[y] \leftarrow R[y] \langle op \rangle  R[x]
\end{equation}
$R$ represents the registers, $x$ and $y$ represent the and target register indices respectively, and $\langle op \rangle$ represents the operation to be performed. In our case, we only allow four operations; addition $(+)$,
subtraction $(-)$, multiplication $(\times)$ and division $(\div)$ by $2$. Division is done by a non-zero constant to prevent division by zero errors that would likely crash the system. This representation makes it easy to implement variation operations, but also easy to digest for humans unlike other machine learning techniques and tree-based programs, which convolute the internal process used to solve a problem. The size of the register set consists of $H_{n\_actions} +  H_{n\_extras}$.

\subsection{The Algorithm}
In this work, we present a linear genetic programming (LGP) approach to evolve a population of programs to solve a given task. The core algorithm, as well as supporting operations, are outlined below.
The core LGP algorithm (Algorithm \ref{alg:core}) starts by initializing a population of programs, where each program is evaluated against the desired task using a suitable fitness function. The programs are then ranked based on their fitness scores, and the least fit individuals are dropped by a given percentage, denoted as $H_{gap}$. The remaining individuals in the population are used to produce new offspring, which fill the dropped spots, thus creating a new generation of the population.
The breeding operation, called Two-Point Crossover (Algorithm \ref{alg:breed}), is used to create offspring by combining parts of two parent programs. It starts by cloning the parent programs, selecting two random points in their instruction sets, and swapping the chunks between them. This operation generates two new offspring. However, only one of the offspring is selected at random and returned, as per the modification.
The mutation operation, called Instruction Replacement (Algorithm \ref{alg:mutate}), is used to introduce random changes into a program. It selects a random instruction from the program and replaces it with a newly generated random instruction. Additionally, it may randomly decide to replace only certain properties (operation, source, or target) of the selected instruction.
The program generation operation (Algorithm \ref{alg:generate}) creates a new program by generating a random number of instructions and a register set, consisting of action registers $H_{n\_actions}$ and working registers $H_{n\_extras}$. The resulting program is then returned.
\begin{algorithm}[!htb]
	\caption{Core: Linear Genetic Programming}
	\label{alg:core}
	\begin{algorithmic}[1]
		\State $P_{0} \gets \Call{InitializePopulation}{H_{population\_size}}$
		\For{$i \in \{0, 1, \ldots, H_{n\_generations}-1\}$}
		\State $\Call{Evaluate}{P_{i}}$
		\State $\Call{Rank}{P_{i}}$
		\State $P_{i+1} \gets \Call{Survive}{P_{i}, H_{gap}}$
		\State $P_{i+1} \gets \Call{Variation}{P_{i+1}}$
		\Comment select individuals through tournament selection and apply variation operations
		\EndFor
		\State \Return $P_{H_{n\_generations} - 1}$
	\end{algorithmic}
\end{algorithm}
\begin{algorithm}[!htb]
	\caption{Breed: Two-Point Crossover}
	\label{alg:breed}
	\begin{algorithmic}[1]
		\State{$P_1', P_2' \gets \Call{Clone}{P_1, P_2}$}
		\State{$I_1 \gets P_1'.instructions$}
		\State{$I_2 \gets P_2'.instructions$}
		\State{$p_1, p_2 \gets \Call{RandomChunk}{I_1, I_2}$}
		\State{$I_1[p_1:p_2], I_2[p_1:p_2] \gets I_2[p_1:p_2], I_1[p_1:p_2]$}
		\State \textbf{return} $\Call{RandomOne}{P_1', P_2'}$
	\end{algorithmic}
\end{algorithm}
\begin{algorithm}[!htb]
	\caption{Mutate: Instruction Replacement}
	\label{alg:mutate}
	\begin{algorithmic}[1]
		\State{$I \gets \Call{RandomInstruction}{P}$}
		\State{$I' \gets \Call{GenerateRandomInstruction}{}$}
		\For{$p \in \{operation, source, target\}$}
		\If{$\Call{Random}{0,1} < 0.5$}
		\State{\Call{ReplaceProperty}{$I, I'$}}
		\EndIf
		\EndFor
		\State{$\Call{ReplaceInstruction}{P, I}$}
		\State \textbf{return} $P$
	\end{algorithmic}
\end{algorithm}
\begin{algorithm}[!htb]
	\caption{Generate: Program Generation}
	\label{alg:generate}
	\begin{algorithmic}[1]
		\State{$n\_instructions \gets \Call{Random}{1, max\_instructions}$}
		\State{$R \gets \Call{GenerateRegisterSet}{H_{n\_actions} + H_{n\_extras}}$}
		\State{$I \gets \Call{GenerateInstructions}{n\_instructions}$}
		\State{$P \gets \Call{CreateProgram}{R, I}$}
		\State \textbf{return} $P$
	\end{algorithmic}
\end{algorithm}

\subsection{Validating The Algorithm}
As a means of ensuring that the framework was implemented correctly, we developed four tests.
Figure \ref{fig:iris_baseline} is a baseline that ensures that the genetic algorithm works with only the reproduction operation.
Figure \ref{fig:iris_crossover}, ensures the two point crossover operation works as expected.
Figure \ref{fig:iris_mutation} ensures that the mutation operation works.
Figure \ref{fig:iris_full} ensures that recombination and mutation can work together to produce a better fitness score than either operation alone.
We also outline the fitness algorithm used for the iris dataset in Algorithm \ref{alg:fitness-iris}.
\begin{algorithm}[!htb]
	\caption{Fitness: Iris}
	\label{alg:fitness-iris}
	\begin{algorithmic}[1]
		\State{$score \gets 0$}
		\For{$I \in inputs$}
		\State{$\Call{Execute}{Program, I}$}
		\State{$pred \gets \Call{Argmax}{Program.Registers[0:H_{n\_actions}]}$}
		\If{$pred = I$}
		\State{$score \pm 1$}
		\EndIf
		\EndFor
		\State{$accuracy \gets \frac{score}{|inputs|}$}
		\State return $accuracy$
	\end{algorithmic}
\end{algorithm}
\begin{figure}[!htb]
	\centering
	\begin{subfloat}[LGP: Iris Reproduction]{\includegraphics[width=\textwidth]{iris_baseline.png}\label{fig:iris_baseline}}
	\end{subfloat}

	\begin{subfloat}[LGP: Iris Recombination]{\includegraphics[width=\textwidth]{iris_crossover.png}\label{fig:iris_crossover}}
	\end{subfloat}

	\caption{Performance comparison of different genetic programming methods on iris dataset}
	\label{fig:iris_comparison}
\end{figure}
\begin{figure}[!htb]
	\ContinuedFloat
	\centering

	\begin{subfloat}[LGP: Iris Mutation]{\includegraphics[width=\textwidth]{iris_mutation.png}\label{fig:iris_mutation}}
	\end{subfloat}

	\begin{subfloat}[LGP: Iris Recombination \& Mutation]{\includegraphics[width=\textwidth]{iris_full.png}\label{fig:iris_full}}
	\end{subfloat}
	\caption{(Continued) Performance comparison of different genetic programming methods on iris dataset}
	\label{fig:iris_comparison_continued}
\end{figure}

\section{OpenAI Gym Integration}
We extend the fitness algorithm to work with a Rust port  of OpenAI's mountain-car-v0 and cartpole-v1 environment \cite{1606.01540}. The code can be viewed at (\url{https://github.com/urmzd/gym-rs}).

For the mountain-car-v0 environment, the agent's goal is to drive a car up a steep hill. The car is subject to gravity and has limited power. The car must learn to rock back and forth to build momentum before reaching the goal area at the top of the hill. The environment has two state variables representing the position, and velocity of the car. The car has three possible actions: push left, push right, or no push. The reward function for the environment gives a value of $-1$ at each time step until the car reaches the goal area, at which point the reward becomes $0$. The problem is considered solved when the car reaches the goal area with an average reward of -110 over 100 consecutive trials.

For the cartpole-v1 environment, the agent's goal is to balance a pole on top of a cart that can move left or right. The pole is subject to gravity, and the goal is to keep the pole upright for as long as possible. The environment has four continuous state variables representing the position and velocity of the cart and pole. The cart has two possible actions: move left or move right. The reward function for the environment gives a value of $1$ at each time step that the pole remains upright. The problem is considered solved when the pole remains upright for at least $195$ consecutive time steps over 100 consecutive trials.

With this in mind, we outline an alternative fitness algorithm used to baseline the framework on classical reinforcement learning problems, Algorithm \ref{alg:fitness-gym}.

\begin{algorithm}[!htb]
	\caption{Fitness: Gym Integration}
	\label{alg:fitness-gym}
	\begin{algorithmic}[1]
		\State{$score \gets 0$}
		\For{$i\in N_{Episodes}$}
		\State{$\Call{Execute}{Program, Environment}$}
		\State{$Action \gets \Call{Argmax}{Program.Registers[0:H_{n\_actions}]}$}
		\State{$Reward, Terminal \gets Sim(Action, Environment)$}
		\If{Terminal}
		\State break
		\EndIf
		\State{$score \pm reward$}
		\EndFor
		\State \textbf{return} $score$
	\end{algorithmic}
\end{algorithm}

\section{Q Learning Integration}
We extend Algorithm \ref{alg:fitness-gym} further to support Q Learning (Algorithm \ref{alg:fitness-q}).
The Q Table consists of a 2D array of size $N_R \times N_A$, where $N_R$ is the number of registers the program can work with and $N_A$ is the number of available actions an agent can take. The Q Table is initialized to all zeros and updates only a different register has been selected. The Q Table is updated using the following formula.
\begin{equation}
	Q(R[x_t], a_t) \leftarrow Q(R[x_t], a_t) + \alpha \left(r_{t+1} + \gamma \max_a Q(R[x_{t+1}], a) - Q(R[x_t], a_t)\right)
\end{equation}
In this formula, $Q$ represents the Q Table, $R$ represents the set of registers, $a$ represents the available actions, $r_{t+1}$ is the reward at time step $t+1$, $\alpha$ is the learning rate, and $\gamma$ is the discount factor. The value $x_t$ represents the current state, while $x_{t+1}$ represents the next state. The update rule calculates the new Q value for the current state-action pair based on the observed reward and the maximum Q value for the next state.
Moreover, during register selection, we apply a greedy selection policy (Algorithm \ref{alg:q-learning-greedy-selection}) with probability $1-\epsilon$. $\epsilon$ exists to allow us to configure the exploration-exploitation factor, i.e., maintain register selection diversity.

\begin{algorithm}[!htb]
	\caption{Q Learning: $\epsilon$-Greedy Selection Policy}
	\label{alg:q-learning-greedy-selection}
	\begin{algorithmic}[1]
		\State $\Call{Execute}{Program, State}$
		\State $WinningRegister \gets \Call{Argmax}{Program.Registers} $
		\State $Action \gets \Call{Argmax}{Q[WinningRegister]}$
		\If{$\Call{Random}{0, 1} < \epsilon$}
		\State $Action \gets \Call{Random}{0, H_{n\_actions}}$
		\EndIf
		\State \textbf{return} $WinningRegister, Action$
	\end{algorithmic}
\end{algorithm}
\begin{algorithm}[!htb]
	\caption{Fitness: Q Learning}
	\label{alg:fitness-q}
	\begin{algorithmic}[1]
		\State $score \gets 0$
		\State $R_{t}, a_{t} \gets \Call{GreedySelection}{Program, Environment}$
		\While{$t < N_{episodes}$}
		\State $r_{t + 1}, Terminal \gets \Call{Sim}{a_{t}, Environment}$
		\State $score \pm r_{t + 1}$
		\If{$\Call{Terminal}{}$}
		\State \textbf{break}
		\EndIf
		\State $R_{t+1}, a_{t+1} \gets \Call{GreedySelection}{Program, Environment}$
		\If{$R_{t} \neq R_{t+1}$}
		\State $\Call{QTableUpdate}{R_{t+1}, a_{t+1}, R_{t}}$
		\EndIf
		\State ${R_{t} \leftarrow R_{t+1}}$
		\State ${a_{t} \leftarrow a_{t+1}}$
		\State ${t \leftarrow t+1}$
		\EndWhile
		\State \Return $score$
	\end{algorithmic}
\end{algorithm}

\section{Experiment Setup}

In this section, we outline the experiment setup used to evaluate the performance of the proposed Q-Learning LGP algorithm in comparison to the baseline LGP. The experiments were conducted in two main stages: hyperparameter optimization and performance evaluation.

\subsection{Hyperparameter Optimization}
First, we employed the Optuna hyperparameter optimization library \cite{akiba2019optuna} to identify the optimal parameters for the baseline LGP programs.
Optuna is a flexible and efficient hyperparameter optimization framework that enables automatic exploration of hyperparameter spaces in pursuit of the best settings for a given algorithm.
After determining the optimal parameters for the baseline LGP programs, we conducted a separate search using Optuna to find the optimal Q-Learning constants for the Reinforced Linear Genetic Programming (RLGP) programs.
This process was aimed at fine-tuning the RLGP algorithm's performance by selecting the most suitable Q-Learning parameters based on the problem domain and the characteristics of the LGP framework.
The resulting parameters were then integrated into the RLGP algorithm by extending the LGP parameters, as shown in \ref{code:cart-pole-parameters}, \ref{code:cart-pole-q-parameters}, \ref{code:mountain-car-parameters}, and \ref{code:mountain-car-q-parameters}.

\begin{listing}[!htb]
	\centering
	\inputminted[breaklines]{json}{../assets/parameters/cart-pole-lgp.json}
	\caption{Cart Pole Parameters}
	\label{code:cart-pole-parameters}
\end{listing}
\begin{listing}[!htb]
	\centering
	\inputminted[breaklines]{json}{../assets/parameters/cart-pole-q.json}
	\caption{Cart Pole Q-Learning Parameters}
	\label{code:cart-pole-q-parameters}
\end{listing}
\begin{listing}[!htb]
	\centering
	\inputminted[breaklines]{json}{../assets/parameters/mountain-car-lgp.json}
	\caption{Mountain Car Parameters}
	\label{code:mountain-car-parameters}
\end{listing}
\begin{listing}[!htb]
	\centering
	\inputminted[breaklines]{json}{../assets/parameters/mountain-car-q.json}
	\caption{Mountain Car Q-Learning Parameters}
	\label{code:mountain-car-q-parameters}
\end{listing}

\subsection{Performance Evaluation}

After obtaining the optimal parameters for both baseline LGP and RLGP, we proceeded with the performance evaluation phase. This stage involved conducting 100 experiments, each consisting of 100 trials. The goal of these experiments was to assess the effectiveness and robustness of the RLGP algorithm compared to the baseline LGP across multiple runs and problem instances.

For each experiment, we calculated the mean, median, minimum, and maximum performance scores obtained by both baseline LGP and RLGP. These summary statistics provided a comprehensive overview of the algorithms' performance, highlighting their strengths and weaknesses across different trials and problem instances.

Finally, we plot the average mean, median, min, max for each experiment to visually compare the performance of the baseline LGP and RLGP algorithms. This visualization allowed us to identify trends and patterns in the algorithms' performance and gain insights into the benefits of incorporating Q-Learning into the LGP framework.

By following this experimental setup, we aimed to provide a thorough and unbiased evaluation of the proposed Q-Learning LGP algorithm, demonstrating its potential advantages over the baseline LGP and paving the way for future research and development in this area.

\chapter{Analysis}

This chapter analyzes the performance of a population of programs trained using Linear Genetic Programming (LGP)
and a population of programs trained using Linear Genetic Programming Reinforced with Q-Learning (LGP-Q). The programs
are trained on the cartpole-v1 and mountain-car-v0 environments \cite{1606.01540} as mentioned in the Reinforcement Integration section.
Once the results are explained, we analyze them to provide insight into the impact of Q-Learning on the performance of LGP. Note that we use the definition of done as referenced on \url{https://github.com/openai/gym/wiki/Leaderboard}.

\section{Experimental Results}

\subsection{Cart Pole}

Figure \ref{fig:cart-pole-comparison} with reference tables \ref{tab:cart-pole-lgp} and \ref{tab:cart-pole-q} show a comparison of the two frameworks performance on the cartpole-v0 task. 
Over 100 experiments, we observe that LGP averages a maximum of 466, a median of 454, a median of 466 and minimum of 128. On the other hand, RLGP
averages a maximum of 213, a median of 207, a mean of 213 a minimum of 31. In both cases,the population of the respective frameworks were able solve the problem. Both frameworks were able
to generate a program in the first 10 generations that was able to solve the problem. However, the RLGP framework immediately plateaus, whilst the LGP framework continues to improve until hitting a plateau at 80 generations.

\begin{figure}[b]
	\centering
	\begin{subfigure}{1.0\textwidth}
		\includegraphics[width=\linewidth]{cart_pole_lgp.png}
		\caption{Performance of LGP on Cart Pole}
		\label{fig:cart-pole-lgp}
	\end{subfigure}
	\hfill
	\begin{subfigure}{1.0\textwidth}
		\includegraphics[width=\linewidth]{cart_pole_q.png}
		\caption{Performance of RLGP on Cart Pole}
		\label{fig:cart-pole-q}
	\end{subfigure}
	\caption{Comparison of RLGP vs LGP Performance on Cart Pole}
	\label{fig:cart-pole-comparison}
\end{figure}


\subsection{Mountain Car}

Figure \ref{fig:mountain-car-comparison} alongside tables \ref{tab:mountain-car-lgp} and \ref{tab:mountain-car-q} demonstrates a similar pattern to that of the cartpole-v0 task. The RLGP framework falls short of solving the task, averaging a maximum of -114, a mean of-130, a median of -117 and a minimum of -200. On the other hand, the LGP framework is able to solve the task, albeit narrowly, averaging a maximum -106, a mean of -126, a median of -120,and a minimum -200. Here, we observe that RLGP achieves a higher median score, with a slight upward trend whereas the LGP achieves a lower median, with no particular trend in any direction. Both of these framework look as they quickly converge to a value and then plateau.

\begin{figure}[b]
	\centering
	\begin{subfigure}{1.0\textwidth}
		\includegraphics[width=\linewidth]{mountain_car_lgp.png}
		\caption{Using LGP on Mountain Car}
		\label{fig:mountain-car-lgp}
	\end{subfigure}
	\hfill
	\begin{subfigure}{1.0\textwidth}
		\includegraphics[width=\linewidth]{mountain_car_q.png}
		\caption{Using RLGP on Mountain Car}
		\label{fig:mountain-car-q}
	\end{subfigure}
	\caption{Comparison of RLGP vs LGP performance on Mountain Car}
	\label{fig:mountain-car-comparison}
\end{figure}

\section{Discussion}

Initially, we would have expected for the frameworks to perform better on mountain-car-v0 than cart-pole-v1 as the state space is much simpler (one consists of 2 state properties whilst the other contains four). However, the opposite seems to be true,
the cart pole task was able to be solved relatively easily in comparison to the mountain car task, in which RLGP failed to solve the problem. One explanation for this discrepancy is the difference in the nature of the tasks. The cart-pole task is inherently more continuous, allowing for small adjustments to have a direct impact on the balancing of the pole. On the other hand, the mountain car task requires more strategic and discrete actions to build up momentum and reach the goal. This may indicate that the LGP and RLGP frameworks are better suited to tackle continuous control problems.

Another contributing factor could be the exploration-exploitation trade-off present in RLGP. The Q-learning component might not be exploring the state space effectively, thus getting stuck in local optima and hindering the overall performance. In contrast, the LGP framework seems to be more robust in its exploration of the solution space, enabling it to perform better on both tasks.

Moreover, the difference in performance may also be attributed to the limitations of the genetic programming approach. While LGP and RLGP are capable of discovering compact and interpretable representations of policies, they are not guaranteed to find the global optimum. The search process depends on the initial population and variation operators, which can impact the quality of the solutions found. The experimental set up might have been poor, and the parameters beneficial to LGP might not have been optimal for RLGP. Its possible that
we could've seen better results if Q learning parameters was not simply a wrapper on top of a preexisting configuration.

In conclusion, our experiments demonstrate that the LGP framework outperforms the RLGP framework in both cart-pole and mountain car tasks. Although both frameworks were able to solve the cart-pole task, RLGP failed to solve the mountain car task, suggesting that the integration of Q-learning into genetic programming may not always lead to improved performance. Future work could involve investigating alternative reinforcement learning algorithms, adapting exploration strategies, or incorporating domain knowledge to enhance the performance of genetic programming-based frameworks. Additionally, it would be valuable to test these frameworks on a broader range of tasks to better understand their strengths and limitations.

\csvautolongtable[
	table head=\caption{LGP Cart Pole Aggregated Results}\label{tab:cart-pole-lgp}\\\hline
	\csvlinetotablerow\\\hline
	\endfirsthead\hline
	\csvlinetotablerow\\\hline
	\endhead\hline
	\endfoot,
	respect all
]{../assets/experiments/aggregate_results/cart_pole_lgp.csv}

\csvautolongtable[
	table head=\caption{RLGP Cart Pole Aggregated Results}\label{tab:cart-pole-q}\\\hline
	\csvlinetotablerow\\\hline
	\endfirsthead\hline
	\csvlinetotablerow\\\hline
	\endhead\hline
	\endfoot,
	respect all
]{../assets/experiments/aggregate_results/cart_pole_q.csv}

 \csvautolongtable[
	 table head=\caption{LGP Mountain Car Aggregated Results}\label{tab:mountain-car-lgp}\\\hline
	 \csvlinetotablerow\\\hline
	 \endfirsthead\hline
	 \csvlinetotablerow\\\hline
	 \endhead\hline
	 \endfoot,
	 respect all
 ]{../assets/experiments/aggregate_results/mountain_car_lgp.csv}

 \csvautolongtable[
	 table head=\caption{RLGP Mountain Car Aggregated Results}\label{tab:mountain-car-q}\\\hline
	 \csvlinetotablerow\\\hline
	 \endfirsthead\hline
	 \csvlinetotablerow\\\hline
	 \endhead\hline
	 \endfoot,
	 respect all
 ]{../assets/experiments/aggregate_results/mountain_car_q.csv}

\chapter{Conclusion}

Due to the amount of time spent on reconfiguring the framework in addition to the short nature of this research project,
various forms of experimentation did not occur. With the further optimization of hyperparameters and a different experimental setup, its possible that results could have improved. Nonetheless, the framework now exists for experimenting with easy, and opens a door for further exploration into the subject at hand. In the future, we would like to
explore different benchmarking approaches and different types of tasks besides those found in the classical control module \cite{1606.01540}, as it might bring forth insight about the types of problems where RLGP can thrive.

\bibliographystyle{plain}
\bibliography{thesis}

\end{document}
